{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLdWfUiFRsuRSMLd+mMYDF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robitussin/CCRNFLRL_EXERCISES/blob/main/Exercise7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš€ Exercise 7\n",
        "\n",
        "In this exercise, you will train a **Deep Q-Network (DQN)** to play the\n",
        "[CartPole-v1 environment](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/) from Gymnasium.\n",
        "\n",
        "`CartPole-v1` is a classic reinforcement learning task where a pole is attached to a cart that moves along a horizontal track . The **goal**: is to keep the pole upright as long as possible  \n",
        "\n",
        "**State Space**:  \n",
        "  1. Cart position  \n",
        "  2. Cart velocity  \n",
        "  3. Pole angle  \n",
        "  4. Pole angular velocity  \n",
        "\n",
        "**Action Space**:  \n",
        "  1. Push cart to the **left**  \n",
        "  2. Push cart to the **right**  \n",
        "\n",
        "**Reward**: +1 for every timestep the pole remains upright  \n",
        "\n",
        "**Episode ends** when:  \n",
        "  - The pole angle exceeds Â±12Â°  \n",
        "  - The cart moves beyond the edges of the track  \n",
        "  - Maximum of 500 timesteps is reached\n"
      ],
      "metadata": {
        "id": "JeqNAxwEKn-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install dependencies\n",
        "\n"
      ],
      "metadata": {
        "id": "JuS1__epJzf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium torch numpy matplotlib imageio[ffmpeg] --quiet"
      ],
      "metadata": {
        "id": "Feog2HdfI90t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Imports\n"
      ],
      "metadata": {
        "id": "BAur-wwaJ6TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from IPython.display import Video\n"
      ],
      "metadata": {
        "id": "SquzwjS9JDPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define the Deep Q-Network\n",
        "\n"
      ],
      "metadata": {
        "id": "YCGeTnctJ86k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# put your answer here"
      ],
      "metadata": {
        "id": "hxmjoOGjJFFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Replay Buffer\n",
        "\n",
        "We store past experiences `(state, action, reward, next_state, done)`  \n",
        "in a buffer and sample random minibatches for training.  \n",
        "This **breaks correlation** between consecutive samples and stabilizes learning.\n"
      ],
      "metadata": {
        "id": "oJW0OCDMKFc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=5000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            torch.FloatTensor(states),\n",
        "            torch.LongTensor(actions),\n",
        "            torch.FloatTensor(rewards),\n",
        "            torch.FloatTensor(next_states),\n",
        "            torch.FloatTensor(dones),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "Fv42M4DuJHoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Training Function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KG5T05tWKLK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(env_name=\"CartPole-v1\",\n",
        "              episodes,\n",
        "              gamma,\n",
        "              lr,\n",
        "              batch_size,\n",
        "              epsilon_decay,\n",
        "              min_epsilon):\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    q_net = DQN(state_size, action_size)\n",
        "    target_net = DQN(state_size, action_size)\n",
        "    target_net.load_state_dict(q_net.state_dict())\n",
        "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
        "\n",
        "    memory = ReplayBuffer(5000)\n",
        "    epsilon = 1.0\n",
        "    rewards_history = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # Îµ-greedy policy\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                state_tensor = torch.FloatTensor(state)\n",
        "                with torch.no_grad():\n",
        "                    action = torch.argmax(q_net(state_tensor)).item()\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # Train network\n",
        "            if len(memory) >= batch_size:\n",
        "                states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
        "                q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "                next_q = target_net(next_states).max(1)[0].detach()\n",
        "                targets = rewards + gamma * next_q * (1 - dones)\n",
        "\n",
        "                loss = nn.MSELoss()(q_values, targets)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "        rewards_history.append(total_reward)\n",
        "\n",
        "        # Update target network periodically\n",
        "        if ep % 50 == 0:\n",
        "            target_net.load_state_dict(q_net.state_dict())\n",
        "            print(f\"Episode {ep}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    return q_net, rewards_history\n"
      ],
      "metadata": {
        "id": "4J87byhmJJ5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Train the Agent"
      ],
      "metadata": {
        "id": "1BGrsADnKQNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparametes\n",
        "\n",
        "# episodes = ?\n",
        "# gamma = ?\n",
        "# lr = ?\n",
        "# batch_size = ?\n",
        "# epsilon_decay = ?\n",
        "# min_epsilon = ?"
      ],
      "metadata": {
        "id": "aTWq7syeUYAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_net, rewards = train_dqn(episodes=episodes, gamma=gamma, lr=lr, batch_size=batch_size, epsilon_decay=epsilon_decay, min_epsilon=min_epsilon)"
      ],
      "metadata": {
        "id": "lGlCFNaOJNEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Record Multiple Episodes as Video\n"
      ],
      "metadata": {
        "id": "1VNjp716KTpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "video_path = \"cartpole_dqn.mp4\"\n",
        "writer = imageio.get_writer(video_path, fps=30)\n",
        "\n",
        "for ep in range(3):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        state_tensor = torch.FloatTensor(state)\n",
        "        with torch.no_grad():\n",
        "            action = torch.argmax(q_net(state_tensor)).item()\n",
        "        state, _, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        frame = env.render()\n",
        "        writer.append_data(frame)\n",
        "\n",
        "writer.close()\n",
        "env.close()\n",
        "print(f\"Video saved to {video_path}\")\n"
      ],
      "metadata": {
        "id": "4rrgHNnVJQt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Watch the video"
      ],
      "metadata": {
        "id": "86aCBXeUKX6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Video(\"cartpole_dqn.mp4\", embed=True)"
      ],
      "metadata": {
        "id": "n7LeD0GWJmO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Evaluate Agent"
      ],
      "metadata": {
        "id": "14kLY7wROnSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, q_net, episodes=20):\n",
        "    total_rewards = []\n",
        "    for _ in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state)\n",
        "            with torch.no_grad():\n",
        "                action = torch.argmax(q_net(state_tensor)).item()\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            ep_reward += reward\n",
        "        total_rewards.append(ep_reward)\n",
        "    return np.mean(total_rewards)\n",
        "\n",
        "eval_env = gym.make(\"CartPole-v1\")\n",
        "avg_reward = evaluate_agent(eval_env, q_net, episodes=20)\n",
        "print(f\"Average reward over 20 eval episodes: {avg_reward:.2f}\")\n",
        "eval_env.close()\n"
      ],
      "metadata": {
        "id": "stKPmInMN9QF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}